steps:
  - name: debian:11-slim
    args:
      - '-c'
      - |
        set -x
        find . | sed -e "s/[^-][^\/]*\// |/g" -e "s/|\([^ ]\)/|-\1/"
        ls /usr/bin/python*
        which python3
        cat /etc/*-release
        DEBIAN_FRONTEND=noninteractive apt-get update && apt-get install -y tzdata sudo curl python-is-python3
        source scripts/install_development_dependencies.sh --yes
        echo "common
        --remote_cache=https://storage.googleapis.com/dataplex-clouddq-bazel-cache"
        >> .bazelrc
        echo "common --google_default_credentials" >> .bazelrc
        env
        make addlicense
        make check
        make test-pip-install
        make build
        make test
        source scripts/install_gcloud.sh
        echo gsutil cp clouddq_patched.zip gs://$_GCS_BUCKET_NAME/build-artifacts/debian11/python3.9/`date -I`/${SHORT_SHA}_`date -Iseconds`/clouddq-executable.zip
        echo gsutil cp clouddq_patched.zip.hashsum gs://$_GCS_BUCKET_NAME/build-artifacts/debian11/python3.9/`date -I`/${SHORT_SHA}_`date -Iseconds`/clouddq-executable.zip.hashsum
    entrypoint: /bin/bash
    env:
    - 'GOOGLE_CLOUD_PROJECT=${_GOOGLE_CLOUD_PROJECT}'
    - 'CLOUDDQ_BIGQUERY_DATASET=${_CLOUDDQ_BIGQUERY_DATASET}'
    - 'CLOUDDQ_BIGQUERY_REGION=${_CLOUDDQ_BIGQUERY_REGION}'
    - 'GCS_BUCKET_NAME=${_GCS_BUCKET_NAME}'
    - 'DATAPLEX_LAKE_NAME=${_DATAPLEX_LAKE_NAME}'
    - 'DATAPLEX_REGION_ID=${_DATAPLEX_REGION_ID}'
    - 'DATAPLEX_ENDPOINT=${_DATAPLEX_ENDPOINT}'
    - 'DATAPLEX_TARGET_BQ_DATASET=${_DATAPLEX_TARGET_BQ_DATASET}'
    - 'DATAPLEX_TARGET_BQ_TABLE=${_DATAPLEX_TARGET_BQ_TABLE}'
    - 'DATAPLEX_TASK_SA=${_DATAPLEX_TASK_SA}'
timeout: 7200s
logsBucket: 'gs://dataplex-clouddq-github-cloud-build'
options:
  logStreamingOption: STREAM_ON
  pool:
    name: >-
      projects/dataplex-clouddq/locations/europe-west6/workerPools/cloud-build-private-pool
serviceAccount: 'projects/dataplex-clouddq/serviceAccounts/github-action@dataplex-clouddq.iam.gserviceaccount.com'